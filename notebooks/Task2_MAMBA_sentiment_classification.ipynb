{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOXB1IVLhaS5CuweffKM7gW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/p20230445-bits/crux-inductions-2025/blob/main/notebooks/Task2_MAMBA_sentiment_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install dependencies"
      ],
      "metadata": {
        "id": "28FerdMUykx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets --quiet\n"
      ],
      "metadata": {
        "id": "fqF0ZbpKxftL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports & device setup"
      ],
      "metadata": {
        "id": "Squrm8mxyuYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", DEVICE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i5TExUXyrCv",
        "outputId": "b135f1b1-a071-401a-8aac-5a5497ee42e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load IMDb dataset"
      ],
      "metadata": {
        "id": "v0ERtZMUy2tL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "train_raw = list(zip(dataset[\"train\"][\"label\"], dataset[\"train\"][\"text\"]))\n",
        "test_raw  = list(zip(dataset[\"test\"][\"label\"], dataset[\"test\"][\"text\"]))\n",
        "\n",
        "# Create validation split (10% of training data)\n",
        "random.shuffle(train_raw)\n",
        "split = int(0.9 * len(train_raw))\n",
        "train_pairs = train_raw[:split]\n",
        "valid_pairs = train_raw[split:]\n",
        "\n",
        "print(f\"Train: {len(train_pairs)}, Valid: {len(valid_pairs)}, Test: {len(test_raw)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-N3l02adyxNM",
        "outputId": "8a5e55f2-2b0b-4249-df4f-b8a52fad2f59"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 22500, Valid: 2500, Test: 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build vocabulary"
      ],
      "metadata": {
        "id": "ZRwiY1ndy-19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MIN_FREQ = 2\n",
        "PAD_TOKEN = \"<pad>\"\n",
        "UNK_TOKEN = \"<unk>\"\n",
        "\n",
        "def tokenize(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "counter = Counter()\n",
        "for _, text in train_pairs:\n",
        "    counter.update(tokenize(text))\n",
        "\n",
        "vocab = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n",
        "for word, freq in counter.items():\n",
        "    if freq >= MIN_FREQ:\n",
        "        vocab[word] = len(vocab)\n",
        "\n",
        "PAD_IDX = vocab[PAD_TOKEN]\n",
        "UNK_IDX = vocab[UNK_TOKEN]\n",
        "\n",
        "print(f\"Vocab size: {len(vocab)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mk5V-1Aey4f5",
        "outputId": "5506fe62-f076-4367-90dd-d10ba49da750"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 94032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encode & collate"
      ],
      "metadata": {
        "id": "ZCVcSgvpzE1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 100\n",
        "\n",
        "def encode(text):\n",
        "    tokens = tokenize(text)\n",
        "    ids = [vocab.get(tok, UNK_IDX) for tok in tokens]\n",
        "    return ids[:MAX_LEN]\n",
        "\n",
        "class IMDbDataset(Dataset):\n",
        "    def __init__(self, pairs):\n",
        "        self.pairs = pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label, text = self.pairs[idx]\n",
        "        ids = encode(text)\n",
        "        return torch.tensor(ids, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "def collate_batch(batch):\n",
        "    ids_list, labels = zip(*batch)\n",
        "    lengths = torch.tensor([len(x) for x in ids_list], dtype=torch.long)\n",
        "    max_len = max(lengths).item()\n",
        "    padded = torch.full((len(ids_list), max_len), PAD_IDX, dtype=torch.long)\n",
        "    for i, ids in enumerate(ids_list):\n",
        "        padded[i, :len(ids)] = ids\n",
        "    return padded, torch.stack(labels), lengths\n"
      ],
      "metadata": {
        "id": "8Mx5PX4gzBT8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data loaders"
      ],
      "metadata": {
        "id": "nf2Ef8wyzJWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_loader = DataLoader(IMDbDataset(train_pairs), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "valid_loader = DataLoader(IMDbDataset(valid_pairs), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
        "test_loader  = DataLoader(IMDbDataset(test_raw), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n"
      ],
      "metadata": {
        "id": "4wJpEpoUzG5I"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simplified MAMBA-style block"
      ],
      "metadata": {
        "id": "Lu1pwT_CzNiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleMambaBlock(nn.Module):\n",
        "    def __init__(self, d_model, hidden_mult=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        inner = hidden_mult * d_model\n",
        "        self.in_proj = nn.Linear(d_model, inner)\n",
        "        self.gate_proj = nn.Linear(d_model, inner)\n",
        "        self.alpha_proj = nn.Linear(d_model, inner)\n",
        "        self.beta_proj = nn.Linear(d_model, inner)\n",
        "        self.out_proj = nn.Linear(inner, d_model)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        B, T, D = x.shape\n",
        "        h = torch.zeros(B, self.in_proj.out_features, device=x.device)\n",
        "        x_in = self.in_proj(x)\n",
        "        x_gate = self.gate_proj(x)\n",
        "        x_alpha = self.alpha_proj(x)\n",
        "        x_beta = self.beta_proj(x)\n",
        "        outs = []\n",
        "        for t in range(T):\n",
        "            gate_t = torch.sigmoid(x_gate[:, t, :])\n",
        "            alpha_t = torch.sigmoid(x_alpha[:, t, :])\n",
        "            beta_t = F.softplus(x_beta[:, t, :])\n",
        "            inp_t = x_in[:, t, :]\n",
        "            h = gate_t * (alpha_t * h + beta_t * inp_t) + (1 - gate_t) * inp_t\n",
        "            outs.append(h)\n",
        "        y = torch.stack(outs, dim=1)\n",
        "        y = self.out_proj(y)\n",
        "        y = self.dropout(y)\n",
        "        y = self.norm(x + y)\n",
        "        return y\n"
      ],
      "metadata": {
        "id": "DH5Niv7xzLMB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mamba sentiment classifier"
      ],
      "metadata": {
        "id": "FMeu8xKKzRn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MambaSentimentClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=128, n_layers=2, num_classes=2, pad_idx=0, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "        self.blocks = nn.ModuleList([SimpleMambaBlock(d_model, hidden_mult=2, dropout=dropout) for _ in range(n_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, ids, lengths):\n",
        "        x = self.embed(ids)\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x, lengths)\n",
        "        mask = (ids != PAD_IDX).unsqueeze(-1)\n",
        "        pooled = (x * mask).sum(dim=1) / lengths.clamp(min=1).unsqueeze(-1)\n",
        "        return self.fc(self.dropout(pooled))\n"
      ],
      "metadata": {
        "id": "FUz4iSyAzPco"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Early stopping"
      ],
      "metadata": {
        "id": "dIoNBwjCzbZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.best_acc = 0.0\n",
        "        self.best_state = None\n",
        "        self.stop = False\n",
        "\n",
        "    def step(self, val_acc, model):\n",
        "        if val_acc > self.best_acc:\n",
        "            self.best_acc = val_acc\n",
        "            self.best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.stop = True\n"
      ],
      "metadata": {
        "id": "CCJzYnk1zYLS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train & evaluate"
      ],
      "metadata": {
        "id": "o4yu4m59zfre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def run_eval(model, loader):\n",
        "    model.eval()\n",
        "    total, correct = 0, 0\n",
        "    for ids, labels, lengths in loader:\n",
        "        ids, labels, lengths = ids.to(DEVICE), labels.to(DEVICE), lengths.to(DEVICE)\n",
        "        logits = model(ids, lengths)\n",
        "        preds = logits.argmax(dim=-1)\n",
        "        total += labels.numel()\n",
        "        correct += (preds == labels).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "def train_model(model, train_loader, valid_loader, epochs=10, lr=2e-3, weight_decay=1e-4, patience=3):\n",
        "    model.to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    early = EarlyStopping(patience=patience)\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        running_loss, running_acc, steps = 0, 0, 0\n",
        "        for ids, labels, lengths in train_loader:\n",
        "            ids, labels, lengths = ids.to(DEVICE), labels.to(DEVICE), lengths.to(DEVICE)\n",
        "            logits = model(ids, lengths)\n",
        "            loss = criterion(logits, labels)\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step()\n",
        "            running_loss += loss.item()\n",
        "            running_acc += (logits.argmax(dim=-1) == labels).float().mean().item()\n",
        "            steps += 1\n",
        "        val_acc = run_eval(model, valid_loader)\n",
        "        print(f\"Epoch {ep} | Train Loss: {running_loss/steps:.4f} | Train Acc: {running_acc/steps:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "        early.step(val_acc, model)\n",
        "        if early.stop:\n",
        "            print(f\"Early stopping triggered at epoch {ep}\")\n",
        "            break\n",
        "    if early.best_state:\n",
        "        model.load_state_dict(early.best_state)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "VUYSXVHtzdWN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run training & test"
      ],
      "metadata": {
        "id": "IgwvoSHZzk7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = len(vocab)\n",
        "model = MambaSentimentClassifier(VOCAB_SIZE, d_model=128, n_layers=2, num_classes=2, pad_idx=PAD_IDX, dropout=0.2)\n",
        "\n",
        "trained_model = train_model(model, train_loader, valid_loader, epochs=3, lr=2e-3, patience=2)\n",
        "\n",
        "test_acc = run_eval(trained_model, test_loader)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4xeD-dNzhPk",
        "outputId": "6542fc1e-ceb7-40be-ff57-f3caa4f6f6f0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Train Loss: 0.5529 | Train Acc: 0.7109 | Val Acc: 0.7768\n",
            "Epoch 2 | Train Loss: 0.3418 | Train Acc: 0.8527 | Val Acc: 0.8148\n",
            "Epoch 3 | Train Loss: 0.1749 | Train Acc: 0.9322 | Val Acc: 0.8116\n",
            "Test Accuracy: 0.7950\n"
          ]
        }
      ]
    }
  ]
}